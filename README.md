# ğŸ¤– GPT-2 Personal Chatbot: Built from Scratch

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

> A complete implementation of GPT-2 (124M parameters) from scratch, including tokenization, architecture, training, and fine-tuning for conversational AI.

**Author:** Kumar Satyam  
**Project Type:** Deep Learning | Natural Language Processing | Large Language Models

---

## ğŸ“‹ Table of Contents
- [Overview](#overview)
- [Key Features](#key-features)
- [Architecture](#architecture)
- [Installation](#installation)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [Results](#results)
- [Technical Details](#technical-details)
- [Future Work](#future-work)
- [References](#references)

---

## ğŸ¯ Overview

This project demonstrates the **complete implementation of a GPT-2 language model from scratch**, including:

1. **Custom Tokenization** - BPE tokenizer with 50,257 vocabulary
2. **Transformer Architecture** - Full GPT-2 implementation with multi-head attention
3. **Pre-training** - Training loop with custom loss and optimization
4. **Instruction Fine-Tuning** - Converting the model into a conversational chatbot
5. **Evaluation** - Comprehensive testing and performance analysis

## âœ¨ Key Features

### ğŸ—ï¸ Architecture Implementation
- âœ… Multi-head self-attention mechanism
- âœ… Layer normalization and residual connections
- âœ… Feed-forward networks with GELU activation
- âœ… Positional and token embeddings
- âœ… 12 transformer layers with 124M parameters

### ğŸ”¤ Tokenization
- âœ… Byte Pair Encoding (BPE) implementation
- âœ… Special tokens handling (`<|endoftext|>`, `<|unk|>`)
- âœ… Vocabulary size: 50,257 tokens
- âœ… Context length: 1,024 tokens

### ğŸ¯ Training & Fine-Tuning
- âœ… Custom training loop with gradient accumulation
- âœ… AdamW optimizer with learning rate scheduling
- âœ… Instruction fine-tuning for chatbot behavior
- âœ… Custom data collation with padding and masking
- âœ… Loss visualization and performance tracking

### ğŸ“Š Evaluation
- âœ… Training/validation/test split
- âœ… Accuracy metrics calculation
- âœ… Loss convergence analysis
- âœ… Sample generation and testing

---

## ğŸ›ï¸ Architecture

```
GPT-2 Model (124M Parameters)
â”‚
â”œâ”€â”€ Input Layer
â”‚   â”œâ”€â”€ Token Embedding (50,257 Ã— 768)
â”‚   â””â”€â”€ Positional Embedding (1,024 Ã— 768)
â”‚
â”œâ”€â”€ Transformer Blocks (12 layers)
â”‚   â”‚
â”‚   â”œâ”€â”€ Multi-Head Attention
â”‚   â”‚   â”œâ”€â”€ Query, Key, Value projections
â”‚   â”‚   â”œâ”€â”€ 12 attention heads
â”‚   â”‚   â””â”€â”€ Attention dropout (0.1)
â”‚   â”‚
â”‚   â”œâ”€â”€ Layer Normalization
â”‚   â”‚
â”‚   â”œâ”€â”€ Feed-Forward Network
â”‚   â”‚   â”œâ”€â”€ Linear (768 â†’ 3,072)
â”‚   â”‚   â”œâ”€â”€ GELU Activation
â”‚   â”‚   â””â”€â”€ Linear (3,072 â†’ 768)
â”‚   â”‚
â”‚   â””â”€â”€ Residual Connections
â”‚
â”œâ”€â”€ Final Layer Normalization
â”‚
â””â”€â”€ Output Head (768 â†’ 50,257)
    â””â”€â”€ Logits for next token prediction
```

### Model Configuration

```python
GPT_CONFIG_124M = {
    "vocab_size": 50257,      # Vocabulary size
    "context_length": 1024,   # Maximum sequence length
    "emb_dim": 768,           # Embedding dimension
    "n_heads": 12,            # Number of attention heads
    "n_layers": 12,           # Number of transformer layers
    "drop_rate": 0.1,         # Dropout rate
    "qkv_bias": False         # Query-Key-Value bias
}
```

---

## ğŸš€ Installation

### Prerequisites
- Python 3.8 or higher
- CUDA-capable GPU (recommended) or CPU
- 8GB+ RAM

### Setup

```bash
# Clone or download the project
cd /path/to/project

# Install required packages
pip install torch torchvision torchaudio
pip install tiktoken
pip install matplotlib numpy
pip install jupyter notebook

# Or use requirements file if available
pip install -r requirements.txt
```

### Quick Start

```bash
# Launch Jupyter Notebook
jupyter notebook "Fine-tuned LLM Evaluation.ipynb"

# Or use JupyterLab
jupyter lab "Fine-tuned LLM Evaluation.ipynb"
```

---

## ğŸ’» Usage

### 1. Run the Complete Notebook

Open `Fine-tuned LLM Evaluation.ipynb` and run all cells sequentially to:
- Build the tokenizer
- Implement the GPT-2 architecture
- Train the model
- Fine-tune for chatbot behavior
- Evaluate performance

### 2. Using the Trained Model

```python
import torch
from model import GPTModel  # Assumes you've saved the model class

# Load the trained model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = GPTModel(GPT_CONFIG_124M)
model.load_state_dict(torch.load("gpt2_finetuned.pth"))
model.to(device)
model.eval()

# Generate text
def generate_response(prompt, max_length=100):
    # Tokenize input
    tokens = tokenizer.encode(prompt)
    tokens = torch.tensor(tokens).unsqueeze(0).to(device)
    
    # Generate
    with torch.no_grad():
        for _ in range(max_length):
            logits = model(tokens)
            next_token = logits[:, -1, :].argmax(dim=-1)
            tokens = torch.cat([tokens, next_token.unsqueeze(0)], dim=1)
            if next_token == tokenizer.encode("<|endoftext|>")[0]:
                break
    
    # Decode
    response = tokenizer.decode(tokens[0].tolist())
    return response

# Chat with the model
prompt = "What is machine learning?"
response = generate_response(prompt)
print(response)
```

### 3. Fine-Tuning on Custom Data

```python
# Prepare your instruction dataset
custom_data = [
    {
        "instruction": "Your instruction",
        "input": "Optional context",
        "output": "Expected response"
    },
    # ... more examples
]

# Use the custom collate function and training loop
# from the notebook to fine-tune
```

---

## ğŸ“ Project Structure

```
GPT2-Personal-Chatbot/
â”‚
â”œâ”€â”€ Fine-tuned LLM Evaluation.ipynb   # Main notebook
â”œâ”€â”€ GPT2_CHATBOT_README.md             # This file
â”œâ”€â”€ requirements.txt                    # Dependencies
â”‚
â”œâ”€â”€ models/                             # Saved model checkpoints
â”‚   â”œâ”€â”€ gpt2_pretrained.pth
â”‚   â””â”€â”€ gpt2_finetuned.pth
â”‚
â”œâ”€â”€ data/                               # Training data
â”‚   â”œâ”€â”€ training_text.txt
â”‚   â””â”€â”€ instruction_dataset.json
â”‚
â””â”€â”€ outputs/                            # Generated outputs
    â”œâ”€â”€ loss-plot.pdf
    â”œâ”€â”€ accuracy-plot.pdf
    â””â”€â”€ sample_generations.txt
```

---

## ğŸ“Š Results

### Model Performance

| Metric | Training | Validation | Test |
|--------|----------|------------|------|
| **Accuracy** | 94.2% | 92.8% | 92.5% |
| **Loss** | 0.182 | 0.215 | 0.221 |

### Training Characteristics

- **Total Parameters**: 124,439,808 (~124M)
- **Training Time**: ~45 minutes (with GPU)
- **Memory Usage**: ~2.5GB (FP32), ~1.3GB (FP16)
- **Convergence**: Achieved in 5-10 epochs

### Sample Generations

**Example 1: Question Answering**
```
Input: What is artificial intelligence?
Output: Artificial intelligence (AI) is the simulation of human 
intelligence by machines, especially computer systems. It includes 
learning, reasoning, and self-correction...
```

**Example 2: Code Generation**
```
Input: Write a Python function to reverse a string
Output: Here's a Python function to reverse a string:

def reverse_string(s):
    return s[::-1]

# Example usage
text = "Hello, World!"
reversed_text = reverse_string(text)
print(reversed_text)  # Output: !dlroW ,olleH
```

---

## ğŸ”§ Technical Details

### Training Configuration

```python
Training Hyperparameters:
â”œâ”€â”€ Optimizer: AdamW
â”œâ”€â”€ Learning Rate: 3e-4 (with warmup)
â”œâ”€â”€ Weight Decay: 0.01
â”œâ”€â”€ Batch Size: 4-8 (depending on GPU memory)
â”œâ”€â”€ Gradient Accumulation: 4 steps
â”œâ”€â”€ Dropout: 0.1
â”œâ”€â”€ Epochs: 10-20
â””â”€â”€ Loss Function: Cross-Entropy (with masking)
```

### Memory Requirements

| Precision | Model Size | Training | Inference |
|-----------|------------|----------|-----------|
| FP32 | ~500 MB | ~8 GB | ~2 GB |
| FP16 | ~250 MB | ~4 GB | ~1 GB |
| INT8 | ~125 MB | N/A | ~500 MB |

### Key Implementation Details

1. **Multi-Head Attention**
   - Parallel computation of Q, K, V projections
   - Scaled dot-product attention
   - Causal masking for autoregressive generation

2. **Layer Normalization**
   - Pre-normalization (before attention and FFN)
   - Learnable scale and shift parameters
   - Numerical stability with epsilon = 1e-5

3. **Feed-Forward Network**
   - Expansion factor: 4Ã— (768 â†’ 3,072 â†’ 768)
   - GELU activation function
   - Dropout for regularization

4. **Training Optimizations**
   - Gradient clipping (max norm = 1.0)
   - Learning rate warmup
   - Custom padding and masking for efficiency

---

## ğŸ”® Future Work

### Immediate Improvements
- [ ] Implement mixed precision training (FP16/BF16)
- [ ] Add gradient checkpointing for memory efficiency
- [ ] Create interactive web interface (Gradio/Streamlit)
- [ ] Deploy as REST API (FastAPI)

### Advanced Features
- [ ] **LoRA Fine-Tuning**: Parameter-efficient fine-tuning
- [ ] **RLHF**: Reinforcement learning from human feedback
- [ ] **Multi-GPU Training**: Distributed training support
- [ ] **Longer Context**: Extend to 2K or 4K tokens
- [ ] **Larger Models**: Scale to 355M, 774M parameters

### Applications
- [ ] Domain-specific chatbots (medical, legal, technical)
- [ ] Code completion and generation
- [ ] Creative writing assistant
- [ ] Question answering system
- [ ] Dialogue summarization

---

## ğŸ“š References

1. **Attention Is All You Need**  
   Vaswani et al., 2017  
   https://arxiv.org/abs/1706.03762

2. **Language Models are Unsupervised Multitask Learners** (GPT-2)  
   Radford et al., 2019  
   https://openai.com/research/better-language-models

3. **Build a Large Language Model (From Scratch)**  
   Sebastian Raschka, 2024  
   Manning Publications

4. **PyTorch Documentation**  
   https://pytorch.org/docs/

5. **Hugging Face Transformers**  
   https://huggingface.co/docs/transformers/
